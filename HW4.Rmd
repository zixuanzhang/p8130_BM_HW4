---
title: "p8130 HW4 Regression"
author: "Eleanor Zhang"
date: "11/11/2018"
geometry: margin=2cm
output: 
     pdf_document:
         latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(broom)
library(tidyverse)
library(HH)
library(leaps)
```

## Problem 2 Heart disease

We are interested in if there is an association between __total cost__ in dollars diagnosed with heart disease and the __number of ER visits__. Other factors will be adjusted later on.


### a) short description of data and look at the data

```{r read data, message=FALSE, warning=FALSE}
heart_disease <- read_csv("./data/HeartDisease.csv") %>% 
  mutate(gender = as.factor(gender),
         complications = as.factor(complications))
```

__Overview__:  

In this dataset, there are `r nrow(heart_disease)` observations of patients with `r ncol(heart_disease)` variables:  

*  __id__: subscriber id
*  __totalcost__: total cost ($) of claims by subscriber
*  __age__: age of subscribers
*  __gender__: gender of patient (1 = male, 0 = otherwise)
*  __interventions__: total number of interventions or procedures carried out
*  __drugs__: number of tracked drugs prescribed
*  __ERvisits__: number of ER visits
*  __complications__: number of complications that arose during heart disease treatment
*  __comorbidities__: number of co-presence of other diseases
*  __duration__: duration of treament condition (in days)

Based our investigation interest, the main outcome is __total cost__ of subscribers with heart disease and the main predictor is __ERvisits__ (number of ER visits). Other important covariates also need to be considered because they could be confounders or have modifier effects on the association relationship between our main predictor and main outcome, including age, interventions, drugs used, complications, and duration of disease. We will first take a look at the availible variables:

i) First we took a look at the distribution of each variable of interest

```{r number summary}
variable_set1 <- dplyr::select(heart_disease, totalcost, ERvisits, everything(), 
                               -c(id, gender, complications))
variable_set2 <- dplyr::select(heart_disease, gender, complications)
knitr::kable(summary(variable_set1))
knitr::kable(summary(variable_set2))
```

Visualize the distribution of variables:

```{r, fig.width=10}
par(mfrow = c(1,2))
hist(variable_set1$totalcost, main = "histogram of total cost (outcome)")
hist(variable_set1$ERvisits, main = "histogram of ER visits")
```

__Describe the main outcome and main predictor__:  

Since total cost and ER visits are both heavily right skewed on the histograms, we better use median and IQR in the summay table to describe them. Especially for total cost, there are many extreme values at the right tail end which needed to be investigated further in the following analysis. We categorized two other remaining variables gender and complications as categorical variables. From the summary table, we saw 

```{r, fig.width=10}
par(mfrow = c(2,3))
hist(variable_set1$age, main = "histogram of age")
hist(variable_set1$interventions, main = "histogram of interventions")
hist(variable_set1$drugs, main = "histogram of drugs")
hist(variable_set1$comorbidities, main = "histogram of comordities")
hist(variable_set1$duration, main = "histogram of duration")
```

__Describe other covariables__:  

Age is slightly left skewed which means elder people have been overly sampled. The median of intervention is about 5 with large IQR = 5. Number of tracked drugs are right skewed so we better make it a categorical variable. Commordities have median of 3.7 with large IQR = 5. Duration of heart disease is roughly uniformly distributed from 50 to 350 days with median 165 days and IQR 240 days. Therefore, these co-variables are not normally distributed in the sample. We have categorized two other remaining variables gender and complications as categorical variables. From the summary table, we saw males are undersampled and other sexs are oversampled (608). The majority of patients do not have any complications.

### b) investigate the shape of distribution for total cost

First we examined the distribution of raw data of total cost and check its normality:

```{r}
par(mfrow = c(1,2))
hist(heart_disease$totalcost, main = "histogram of total cost")
qqnorm(heart_disease$totalcost)
```

Then we try __log transformation__ on totalcost to see if this will improve the normality.

```{r}
heart_disease <- mutate(heart_disease, log_totalcost = log(totalcost))
par(mfrow = c(1,2))
hist(heart_disease$log_totalcost, main = "histogram of log total cost")
heart_disease$log_totalcost[is.infinite(heart_disease$log_totalcost)] = 0.001
qqnorm(heart_disease$log_totalcost)
qqline(heart_disease$log_totalcost, col = "red", lwd = 2)
```

Comment: After log transformation, we saw a pretty good bell shaped ditribution. So we will use this transformed data in the linear model fitting and interpretation.


### c) dichotomize complications

0 represents no complications; 1 represents having complications

```{r fig.width=5, fig.height=3}
heart_disease <- heart_disease %>% 
  mutate(comp_bin = ifelse(complications == 0, 0, 1))
heart_disease %>% ggplot(aes(x = comp_bin)) + geom_bar()
```

### d) fit linear model SLR 

From part (b), we saw the transformed data look better in normal shape, we will use the transformed data to fit SLR. So we fit a simple linear regression model between outcome __log_totalcost__ and predictor __ERvisits__. Let $Y_{i}$ = response(total cost), $X_{i}$ = predictor (ERvisits).

Then our model is $logY_{i} = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$. Here we assume the error is normally distribued. then $\epsilon_{i} \sim N(0, \sigma^2)$

```{r SLR}
SLR <- lm(log_totalcost ~ ERvisits, data = heart_disease)
summary(SLR)
plot(heart_disease$ERvisits, heart_disease$log_totalcost)
abline(SLR, col = "blue", lwd = 2)
```

The result of regression tells that the fitted model is :

$$\hat{logY_{i}} = 5.517 + 0.23X_{i}$$

__Results and Interpretation__: The fitted model indicates that for every unit increase in ER visits, the expected total cost in dollars _on logarithm scale_ will increase by 0.23, or 1.26 on original scale. When the ER visit is 0, the expected total cost in dollar _on logarithm scale_ will be 5.517, or 249 dollars. The p value for two estimators $\beta_{0}$ and $\beta_{1}$ are well below 0.001. So we are very confident that there is a strong association between total cost and ER visits, and our simple regression model describes their relationship.

### e) fit MLR with comp_bin and ERvisits

i) test if __comp_bin__ is an effect modifier of the relationship between __totalcost__ and __ERvisits__  

Let $Y_{i}$ = response(total cost), $X_{i1}$ = predictor (ERvisits), $X_{i2}$ = comp_bin(factor with two levels)

The full model is :  

$logY_{i} = \beta_0 + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$  

Now add a potential modifier (interaction):  

$logY_{i} = \beta_0 + \beta_{1}X_{i1} + \beta_{2}X_{i2}+ \beta_{3}X_{i3}X_{i3} + \epsilon_{i}$

Our hypothesis statement is:  $H_{0}: \beta_{3} = 0$ vs. $H_{a}: \beta_{3} \ne 0$

```{r modifier effect}
MLR_comp <- lm(log_totalcost ~ ERvisits + comp_bin, data = heart_disease)
MLR_comp_inter <- lm(log_totalcost ~ ERvisits + comp_bin + ERvisits*comp_bin, data = heart_disease)
summary(MLR_comp) 
summary(MLR_comp_inter)
anova(MLR_comp, MLR_comp_inter) %>% tidy
```


Comment: Based on the regression summary and anova result comparing two models, p value for the interaction coefficient $\beta_{3}$ is 0.314, which is quite large. Anova F test for comparing two models indicate adding the interaction term does not increase SSR by significant amount. The adjusted $R^2$ is the same for these nested models. Therefore at 0.95 significance level, we do not have evidence to reject the null. Hence there is no significant interaction or modifier effect of complications in the relationship between total cost and ER visits.

We can also visualize this interaction model:  

```{r interaction model}
range(heart_disease$ERvisits)
ER <- seq(0,20,0.5)
beta <- MLR_comp_inter$coefficients
# comp_bin = 0
yhat1 <- beta[1] + beta[2]*ER 
# comp_bin = 1
yhat2 <- beta[1] + beta[3] + (beta[2] + beta[4])*ER

plot(heart_disease$ERvisits, heart_disease$log_totalcost, 
     main = "(log) total cost with complications and no complications",
     xlab = "")
lines(ER, yhat1, col = 2, lwd = 2) # total cost of comp_bin = 0 with fixed ER
lines(ER, yhat2, col = 3, lwd = 2) # total cost of comp_bin greater than 0 with fixed ER
```

Comment: although we expect two parallel lines on the plot if there is truly no interaction effect. However, the statistical test we conducted above indicate that although there is some interaction effect, the effect is not significant. As a conclusion, we will not consider this mediator effect.

ii) test if __comp_bin__ is a confounder of relationship between total cost and ERvisits

Model 1 without comp_bin:  $logY_{i} = \beta_0 + \beta_{1}X_{i1} + \epsilon_{i}$  

Model 2 with comp_bin:  $logY_{i} = \beta_0 + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \epsilon_{i}$

```{r}
SLR <- lm(log_totalcost ~ ERvisits, data = heart_disease)
MLR_comp <- lm(log_totalcost ~ ERvisits + factor(comp_bin), data = heart_disease)
summary(SLR) 
summary(MLR_comp)
anova(SLR, MLR_comp) %>% tidy
```

Comment: From the regression result, we saw the coefficient of __comp_bin__ is quite significant with p value well below 0.001. If we calculate the F statistics for this nested model: $F = \frac{(SSR_{large} - SSR_{small})/1}{SSE_{large}/785} = \frac{116.6}{2464.668/785} = 37.14 \sim F(1, 785)$, then we will reject the null at 0.05 level since this test statistics is very large. In addition, the adjusted $R^2$ increased when adjusting for __comp_bin__. The anova result for two model comparison shows that adding complication variable greatly reduce the overall SSTO while increasing SSR, with p value well below 0.001. So we should include complications in our linear model.

iii) decide if comp_bin should be included along with ERvisits

From above test, we should include __comp_bin__ as a predictor in our additive linear model. Adding __com_bin__ in the model increase SSR significantly. The coefficient of __comp_bin__ is also significant in the linear model from above discussion.

### f) examine additional covariates 

(i)  fit a MLR  

We start with screening for any colinearility of variables among __ERvisits__, __comp_bin__, __age__, __gender__, and __duration__

```{r}
heart_disease %>% dplyr::select(log_totalcost, ERvisits, comp_bin, age, gender, duration) %>% pairs()
```

Then we fit a MLR with all variables of interests: 

```{r}
fit_all <- lm(log_totalcost ~ ERvisits + comp_bin + age + factor(gender) + duration, 
              data = heart_disease)
vif(fit_all)
summary(fit_all)
anova(fit_all) 
```

__Comment__: From the VIF test, there is no significant colinearity between predictors. Based on the t test statistics for each regression coefficients along with their p values, we observed linear relationship between gender and total cost is weak. The adjusted $R^2$ for this full model is about 0.26. Then we performed ANOVA test. From F test for each nested model indicate that we should definitely include __duration__ in our model and better exclude age and gender since they do not much additional information in the exisiting model. But we still need to decide which combinations of predictors will provide the best association between predictors and outcomes. So we run into the stage of model selection:

(ii) compare SLR and MLR  

Here we construct several nested MLR to determine if we want to include the predicor in the model or not.


```{r}
SLR <- lm(log_totalcost ~ ERvisits, data = heart_disease) # start from here
heart <- heart_disease %>% dplyr::select(ERvisits, comp_bin, duration, age, gender,log_totalcost)
```

compare different models using both adjusted $R^2$ and Cp criterion. 

```{r}
MLRs <- regsubsets(log_totalcost ~ ., data=heart)
summary(MLRs)
``` 

So we have the best models for each size of predictors in terms of Cp and adjusted $R^2$, we can build models for each of them

```{r}
modelfit_1 <- lm(log_totalcost ~ duration, data = heart)
modelfit_2 <- lm(log_totalcost ~ ERvisits + duration, data = heart)
modelfit_3 <- lm(log_totalcost ~ ERvisits + comp_bin + duration, data = heart)
modelfit_4 <- lm(log_totalcost ~ ERvisits + comp_bin + duration + age, data = heart)
modelfit_5 <- lm(log_totalcost ~ ERvisits + comp_bin + duration + age + factor(gender), data = heart)
model_result <- tibble(predictors = c(1,2,3,4,5),
       data = list(modelfit_1, modelfit_2, modelfit_3, modelfit_4, modelfit_5))
```

them compare their AIC and BIC

```{r}
compare_models <- model_result %>% mutate(glance_result = map(data, glance)) %>% 
  dplyr::select(-data) %>% 
  unnest() %>% 
  dplyr::select(predictors, AIC, BIC, adj.r.squared)

plot(compare_models$predictors, compare_models$adj.r.squared)                        
```

Comment: the adjusted $R^2$ does not change much after 3 predictors. so we will use the three predictors: ERvisit, comp_bin and duration.



## Problem 3

The investigators wants to test the relationship between patient's satisfaction (Y) and age, severity of illness, and anxiety level. The dataset contains 46 patients observations

### a) correlation matrix

```{r}
pat_sat <- readxl::read_excel("./data/PatSatisfaction.xlsx")
pairs(pat_sat)
cor(pat_sat) %>% knitr::kable()
```

Comment: the correlation matrix shows that age, severity of illness and anxiety level are consistently negatively correlated with satisfaction score. Age seems to have the strongest correlation with satisfaction score while the other variables also have significant coefficient of correlations. However, covariates are positively correlated with each other significantly as well. So we should keep this in mind.

### b) fit a MLR and test whether there is a regression relation

In this MLR model, we will use the satisfaction as response while all other three variables as predictors. 
Let $Y_{i}$ = satisfaction (outcome), $X_{i1}$ = age, $X_{i2}$ = severity of illness, $X_{i3}$ = anxiety level

Full Model: $Y_{i} = \beta_0 + \beta_{1}X_{i1} + \beta_{2}X_{i2} + \beta_{2}X_{i2} + \epsilon_{i}$ 

```{r}
MLR_all <- lm(Safisfaction ~ Age + Severity + Anxiety, data = pat_sat)
summary(MLR_all)
```

First We need to do an overall F test for the three predictors:

State the hypothesis:  
$$H_{0}: \beta_{1} = \beta_{2} = \beta_{3} = 0 \\ H_{a}: \text{at least one of the coefficient is nonzero}$$

Test Statistic: $F_{test} = \frac{MSR}{MSE} = \frac{9120.5/3}{4248.8/42} = 30.05 \sim F(3, 42)$

Decision Rule: at $\alpha = 0.05$, we will reject the null if $F_{test} > F(0.95,3,42) = 2.83$. Here we have $F_{test} = 30.05 > 2.83$, so we should reject the null and conclude that there is at least one linear association among these predictors with the outcome satisfaction level.


### c) compute 95% CI for estimated coefficients

create a table with estimator and 95% Confidence Interval:

```{r}
summary(MLR_all) %>% 
  tidy %>% 
  mutate(lower_bound = estimate - qt(0.975, 42) * std.error,
         upper_bound = estimate + qt(0.975, 42) * std.error) %>% 
  dplyr::select(term, estimate, std.error, lower_bound, upper_bound)
```

Interpret severity of illness: 

While holding age and anxiety level constant, the expected __decrease__ of satisfaction score with an unit increase in severity of illness is 0.442. We are 95% confident that the true mean change of satisfaction score with one unit increase in severity of illness is between -1.43 to 0.551.


### d) Obtain interval estimate for a new patient

```{r}
new_data <- tibble(Age = 35,
                   Severity = 42,
                   Anxiety = 2.1)
predict.lm(MLR_all, new_data, interval="prediction", conf.level = 0.95)
```

Comment: The point estimator for this new patient's satisfaction score is 71.7. We are 95% confident that the predicted satisfaction score for this new patient will be between 50.1 to 93.3

### e) test whether anxiety level can be dropped from the MLR

State hypothesis: $H_{0}: \beta_{3} = 0$ vs. $H_{a}: \beta_{3} \ne 0$

Test statistic:  $F_{test} = \frac{SSR_{X3|X1,X2}/1}{SSE_{X1,X2}/43} = \frac{364.16}{4613/43} = 3.6 \sim F(1, 43)$

Rejection rule: at $\alpha = 0.05$, we should reject null if $F_{test} > F(0.95, 1,43) = 4.07$. However, we obtained $F_{test} = 3.6 < 4.07$, so we do not have evidence to reject the null. Therefore we should not include anxiety level as one of the explaintary variable since it does not reduce SSTO significantly in a model with exisiting variables age and Severity of illness.

Perform test in R:
```{r}
MLR_Age_Sev <- lm(Safisfaction ~ Age + Severity, data = pat_sat)
MLR_all <- lm(Safisfaction ~ Age + Severity + Anxiety, data = pat_sat)
anova(MLR_Age_Sev) %>% tidy
anova(MLR_all) %>% tidy
anova(MLR_Age_Sev, MLR_all) %>% tidy
```




