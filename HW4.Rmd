---
title: "p8130 HW4 Regression"
author: "Eleanor Zhang"
date: "11/11/2018"
output: 
     pdf_document:
         latex_engine: xelatex
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(faraway)
library(broom)
library(tidyverse)
library(HH) 
```

## Problem 2 Heart disease

We are interested in fi there is an association between __total cost__ in dollars diagnosed with heart disease and the __number of ER visits__. Other factors will be adjusted later on.

### a) short description of data and look at the data

```{r}
heart_disease <- read_csv("./data/HeartDisease.csv")
head(heart_disease)
```

In this dataset, there are `r nrow(heart_disease)` observations of patients with `r ncol(heart_disease)` variables:  

*  __id__: patient id
*  __totalcost__: total cost ($) of patients who are diagnosed with heart disease
*  __age__: age of patients
*  __interventions__: number of interventions (integers)
*  __drugs__: ? number of drugs.
*  __ERvisits__: number of ER visits
*  __complications__: number of complications
*  __comorbidities__: number of co-presence of other diseases in additional to heart disease
*  __duration__: duration of heart disease (in days)

Based our investigation interest, the main outcome is __total cost__ of patients with heart disease and the main predictor is __ERvisits__ (number of ER visits). Other important covariates also need to be considered because they could potential have differential effect on the association relationship between out main predictor and main outcome, including age, interventions, drugs used, complications, and duration of disease. We will first take a look at these variables:

i) First we took a look at the main outcome and main predictor

number summaries for variables:
```{r}
variable_set1 <- dplyr::select(heart_disease, totalcost, ERvisits, everything(), -c(id, gender, complications))
variable_set2 <- dplyr::select(heart_disease, gender, complications)
knitr::kable(summary(variable_set1))
table(variable_set2)
#margin.table(table(variable_set2))
#prop.table(table(variable_set2))
```

Visualize the distribution of these variables

```{r}
par(mfrow = c(1,2))
hist(variable_set1$totalcost)
hist(variable_set1$ERvisits)
```

Comment: Since total cost and ER visits are both heavily right skewed on the histograms, we better use median and IQR in the summay table to describe them. Especially for total cost, there are many extreme values at the right tail end which needed to be investigated further in the following analysis.

```{r}
par(mfrow = c(2,3))
hist(variable_set1$age)
hist(variable_set1$interventions)
hist(variable_set1$drugs)
hist(variable_set1$comorbidities)
hist(variable_set1$duration)
```

Comment: age is slightly left skewed which means elder people have been overly sampled. The median of intervention is about 5 with large IQR of 5. drugs?. Commordities have median of 3.7 with large IQR 5. Duration of heart disease is roughly uniformly distributed from 50 to 350 days with median 165 days and IQR 240 days. Therefore, these co-variables are not normally distributed in the sample, so we need to adjust for this in later analysis.

### b) investigate the shape of distribution for total cost

raw data of total cost
```{r}
par(mfrow = c(1,2))
hist(heart_disease$totalcost)
qqnorm(heart_disease$totalcost)
```

Then we try log transformation on __totalcost__ to see if this will transform the distribution.

```{r}
heart_disease <- mutate(heart_disease, log_totalcost = log(totalcost))
par(mfrow = c(1,2))
hist(heart_disease$log_totalcost)
heart_disease$log_totalcost[is.infinite(heart_disease$log_totalcost)] = 0
qqnorm(heart_disease$log_totalcost)
```

comment: After log transformation, we saw a pretty good bell shape of the ditribution. So we will use this transformed data as needed.


### c) dichotomize complications

```{r}
heart_disease <- heart_disease %>% 
  mutate(comp_bin = ifelse(complications == 0, 0, 1))
```

### d) fit linear model SLR

From part (b), we saw the transformed data look better in normal shape, we will use the transformed data to fit SLR. So we fit a simple linear regression model between outcome __log_totalcost__ and predictor __ERvisits__. Let $Y_{i}$ = response(total cost), $X_{i}$ = predictor (ERvisits).

Then our model is $logY_{i} = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$. Here it is reasonable to assume the error is normally distributed because the log transformation improve normality. then assume $\epsilon_{i} \sim N(0, \sigma^2)$

```{r}
SLR <- lm(log_totalcost ~ ERvisits, data = heart_disease)
summary(SLR)
plot(heart_disease$ERvisits, heart_disease$log_totalcost)
abline(SLR, col = "red", lwd = 2)
```

The result of regression tells that the fitted model is :

$$\hat{logY_{i}} = 5.517 + 0.23X_{i}$$

Interpretation: The fitted model indicates that for every unit increase in ER visits, the total cost in dollars on logarithm scale will increase by 0.23. when the ER visit is zero, the total cost in dollar on logarithm scale will be 5.517. The p value for two estimators $\beta_{0}$ and $\beta_{1}$ are well below 0.001. So we are very confident that there is a strong association between total cost and ER visits, and our simple regression model describes their relationship.

### e) fit MLR with comp_bin and ERvisits

i) test if __comp_bin__ is an effect modifier of the relationship between __totalcost__ and __ERvisits__  

Let $Y_{i}$ = response(total cost), $X_{i1}$ = predictor (ERvisits), $X_{i2}$ = {when comp_bin equals 1, otherwise equals 0}

The full model is :  $logY_{i} = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_{i2} + \epsilon_{i}$  

Now add in a potential modifier: $logY_{i} = \beta_0 + \beta_{1}X_{i1} + \beta_{2}X_{i2}+ \beta_{3}X_{i1}X_{i2} + \epsilon_{i}$

So our hypothesis statement is:  $H_{0}: \beta_{3} = 0$ vs. $H_{a}: \beta_{3} \ne 0$  

```{r}
MLR_comp <- lm(log_totalcost ~ ERvisits + comp_bin, data = heart_disease)
MLR_comp_inter <- lm(log_totalcost ~ ERvisits + comp_bin + ERvisits*comp_bin, data = heart_disease)
summary(MLR_comp) %>% tidy
summary(MLR_comp_inter) %>% tidy
anova(MLR_comp, MLR_comp_inter)
```

Based on the regression result, p value for the interaction coefficient $\beta_{3}$ is 0.314, which is quite large. So at 0.95 significance level, we do not have evidence to reject the null. Therefore we is no interaction or modifier effect of complications in the relationship between total cost and ER visits.

To visualize interaction model:  

```{r}
range(heart_disease$ERvisits)
ER <- seq(0,20,0.5)
beta <- MLR_comp_inter$coefficients

# comp_bin = 0
yhat1 <- beta[1] + beta[2]*ER 
# comp_bin greater than 0
yhat2 <- beta[1] + beta[3] + (beta[2] + beta[4])*ER

plot(heart_disease$ERvisits, heart_disease$log_totalcost)
lines(ER, yhat1, col = 2, lwd = 2) # comp_bin = 0
lines(ER, yhat2, col = 3, lwd = 2) # comp_bin greater than 0
```

ii) test if __comp_bin__ is a confounder of relationship between total cost and ERvisits

Model 1 without comp_bin:  $logY_{i} = \beta_0 + \beta_{1}X_{i} + \epsilon_{i}$  

Model 2 with comp_bin:  $logY_{i} = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_{i2} + \epsilon_{i}$

```{r}
SLR <- lm(log_totalcost ~ ERvisits, data = heart_disease)
MLR_comp <- lm(log_totalcost ~ ERvisits + comp_bin, data = heart_disease)
anova(SLR, MLR_comp)
```

iii) decide if comp_bin should be included along with ERvisits

### f) examine additional covariates 

```{r}
heart_disease %>% dplyr::select(log_totalcost, ERvisits, age, gender, duration) %>% pairs()
```


## Problem 3

The investigators wants to test the relationship between patient's satisfaction (Y) and age, severity of illness, and anxiety level. The dataset contains 46 patients observations

### a) correlation matrix

```{r}
pat_sat <- readxl::read_excel("./data/PatSatisfaction.xlsx")
head(pat_sat)
pairs(pat_sat)
hist(pat_sat$Safisfaction)
hist(pat_sat$Age)
hist(pat_sat$Severity)
hist(pat_sat$Anxiety)
cor(pat_sat) %>% knitr::kable()
```

Comment: the correlation matrix shows that age, severity of illness and anxiety level are consistently negatively correlated with satisfaction score. Age seems to have the strongest correlation with satisfaction score while the other variables also have significant coefficient of correlations.

### b) fit a MLR and test whether there is a regression relation

In this MLR model, we will use the satisfaction as response while all other three variables as predictors. 
Let $Y_{i}$ = satisfaction (outcome), $X_{i1}$ = age, $X_{i2}$ = severity of illness, $X_{i3}$ = anxiety level

Full Model: $Y_{i} = \beta_0 + \beta_{1}X_{i} + \beta_{2}X_{i2} + \beta_{2}X_{i2} + \epsilon_{i}$ 

```{r}
names(pat_sat)
MLR_all <- lm(Safisfaction ~ Age + Severity + Anxiety, data = pat_sat)
summary(MLR_all)
anova(MLR_all)
```

First We need to do an overall F test for the three predictors:

```{r}
summary(MLR_all)
```

State the hypothesis: 
$$H_{0}: \beta_{1} = \beta_{2} = \beta_{3} = 0 \\ H_{a}: \text{at least one of the coefficient is nonzero}$$

Test Statistic: $F_{test} = \frac{MSR}{MSE} = 30.05 \sim F(4, 42)$

Decision Rule: at $\alpha = 0.05$, we will reject the null if $F_{test} > F(0.95,4,42) = 2.59$. Here we have $F_{test} = 30.05 > 2.59$, so we should reject the null and conclude that there is at least one linear association among these predictors with the outcome satisfaction level.

Then we can test these predictors one by one: 

1. Test for $\beta_{1}$: the coefficent of age

State hypothesis: $H_{0}: \beta_{1} = 0$ vs. $H_{a}: \beta_{1} \ne 0$ 

Test statisitc: we can use the generalized F test for SLR which is equivalent to t test for single regression coefficient. $F_{test} = \frac{SSR_{L}/1}{SSE_{L}/(46-2)} = 81.8 \sim F(1, 44)$

Decision rule: at $\alpha = 0.05$, we will reject the null if $F_{test} > F(1-\alpha, 2, 44) = 4.06$. Here we have $F_{test} = 81.8 > 4.06 $, with p value well below 0.001, so we will reject the null and conclude there is linear association between satisfaction score and age.

2. Test for $\beta_{2}$: the coefficent of Severity

State hypothesis: $H_{0}: \beta_{2} = 0$ vs. $H_{a}: \beta_{2} \ne 0$ 

Test statisitc: $F_{test} = \frac{SSR_{X2|X1}/1}{SSE_{X1}/44} = 4.75 \sim F(1, 44)$

Rejection rule: we should reject the null if $F_{test} > 4.06$ at $\alpha = 0.05$. Here we have $F_{test} = 4.75 > 4.06$, so we should reject the null at $\alpha = 0.05$ and conclude there is linear association between Severity of illness and satisfaction level.

3. Test for $\beta_{3}$: the coefficent of Anxiety level

State hypothesis: $H_{0}: \beta_{3} = 0$ vs. $H_{a}: \beta_{3} \ne 0$

Test statistic:  $F_{test} = \frac{SSR_{X3|X1,X2}/1}{SSE_{X1,X2}/43} = 3.6 \sim F(1, 43)$

Rejection rule: at $\alpha = 0.05$, we should reject null if $F_{test} > F(0.95, 1,43) = 4.07$. However, we obtained $F_{test} = 3.6 < 4.07$, so we do not have evidence to reject the null. Therefore we should not include anxiety level as one of the explaintary variable since it does not reduce SSE significantly in a model with exisiting variables age and Severity of illness.



